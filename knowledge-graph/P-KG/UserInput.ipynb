{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0fe30298-9f8d-47f1-a91d-2426cbe08f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import datetime as datetime\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import docx2txt\n",
    "from typing import List, Dict, Tuple, Sequence, Optional\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import fitz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a345038-2bfc-4e03-a32e-400cba23f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOKENIZER = None\n",
    "\n",
    "_CLEAN_PUA = re.compile(r'[\\uE000-\\uF8FF]')                 # private-use (e.g., \\uf0b7)\n",
    "_CLEAN_ZW  = re.compile(r'[\\u200B-\\u200D\\uFEFF]')           # zero-widths/BOM\n",
    "_CLEAN_CTRL= re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]')    # other control chars\n",
    "_SOFT_HY   = re.compile(r'\\u00AD')                          # soft hyphen\n",
    "_LINE_HY   = re.compile(r'-\\s*\\n\\s*')                       # hyphen line-break joins\n",
    "_BULLETS   = re.compile(r'[\\u2022\\u00B7]')                  # • or ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba50c4c4-edf7-45d2-a603-5f52d31ae594",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4100cc0-ae8d-4aa6-9a9e-25da3dfd91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iso_time():\n",
    "    now = datetime.now(timezone.utc)\n",
    "    timestamp_iso = now.isoformat(timespec=\"milliseconds\")\n",
    "    timestamp_iso = timestamp_iso.replace(\"+00:00\", \"Z\")\n",
    "    timestamp_ms = int(now.timestamp()*1000)\n",
    "\n",
    "    return timestamp_iso, timestamp_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb304d6-c1ca-4438-93b6-0ef151ff2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6d6ffd6b-487b-4b93-800f-f05f18050527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model():\n",
    "    return SentenceTransformer(MODEL_NAME, \n",
    "                               cache_folder=\"./models_cache\",\n",
    "                               token=None\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "01d7765f-647b-47d0-be6b-142047a7655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_embedding(text, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    model = _get_model()\n",
    "    return model.encode(text, normalize_embeddings=True) #return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1f9713-2ebb-496b-a33d-7efc718a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is an existing row in the input history for user where the same input was already used on the same model\n",
    "#returns row where user_input and model combination already exist\n",
    "def check_existing_input(user_input, prompt_model, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    matches = df.loc[(df['prompt_model'] == prompt_model) & (df['user_input'] == user_input)].copy()\n",
    "\n",
    "    return matches if not matches.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7d9d17-8d41-43c1-9532-874f4312503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_embedding(user_hash, user_input, input_uid, embedding_model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    filename = user_hash+\"_InputEmbeddings.pkl\"\n",
    "    embedding = create_vector_embedding(user_input, embedding_model)\n",
    "\n",
    "    new_row = {\n",
    "        \"uid\" : input_uid,\n",
    "        \"embedding\" : embedding\n",
    "    }\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(new_row)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07aceaf9-20ff-4640-a76e-cb36b8e2c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_input_embeddings(user, embedding_model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    user_hash = get_uid(user)\n",
    "    filename = user_hash+\"_InputHistory.csv\"\n",
    "    temp_file = user_hash+\"InputHistory_temp.csv\"\n",
    "    changed = False\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"File does not exist for user: {user}\")\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        dtype={\n",
    "            \"embedding_exists\" : \"int64\",\n",
    "            \"embedding_model\" : \"string\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['embedding_exists'] == 0:\n",
    "            save_input_embedding(user_hash, row['user_input'], row['input_uid'], embedding_model)\n",
    "            df.at[index, 'embedding_exists'] = 1\n",
    "            df.at[index, 'embedding_model'] = embedding_model\n",
    "            df.to_csv(temp_file, index=False)\n",
    "            changed = True\n",
    "\n",
    "    if changed:\n",
    "        os.replace(temp_file, filename)\n",
    "        print(\"Updates written to file\")\n",
    "\n",
    "    else:\n",
    "        print(\"No updates written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4c333f-f0e7-486f-b6d2-06c67919ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_embeddings(user):\n",
    "    user_hash = get_uid(user)\n",
    "    filename = user_hash+\"_InputEmbeddings.pkl\"\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc672d07-92f1-47f3-8c56-ea98858fb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_user_input(user, user_input, prompt_model):\n",
    "    user_hash = get_uid(user)\n",
    "    input_uid = get_uid(user_input)\n",
    "    filename = user_hash+\"_InputHistory.csv\"\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                'user_input',\n",
    "                'input_uid',\n",
    "                'prompt_model',\n",
    "                'timestamp_iso',\n",
    "                'timestamp_ms',\n",
    "                'embedding_model'\n",
    "            ])\n",
    "\n",
    "    existing_row = check_existing_input(user_input, prompt_model, filename)\n",
    "    \n",
    "    if existing_row is not None:\n",
    "        dt = existing_row['timestamp_ms'].iloc[0]\n",
    "        dt = datetime.fromtimestamp(dt / 1000)\n",
    "        print(f\"Input already used at: {dt}\")\n",
    "        return False\n",
    "\n",
    "    timestamp_iso, timestamp_ms = get_iso_time()\n",
    "\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            user_input,\n",
    "            input_uid,\n",
    "            prompt_model,\n",
    "            timestamp_iso,\n",
    "            timestamp_ms,\n",
    "            MODEL_NAME, #placeholder for vector embedding model used\n",
    "        ])\n",
    "\n",
    "    save_input_embedding(user_hash, user_input, input_uid)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd1702c-ccba-473e-8a29-4334c9451b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tokenizer(name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    global _TOKENIZER\n",
    "    if _TOKENIZER is None:\n",
    "        _TOKENIZER = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "    return _TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e90f42-e4dc-4f07-95f5-fd38064a0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tok_len(tok, s: str) -> int:\n",
    "    return len(tok.encode(s, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d359417-1ed6-459e-99f8-0da78398b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = _LINE_HY.sub('-', text)          # join hyphenated line breaks\n",
    "    text = _SOFT_HY.sub('', text)           # drop soft hyphen\n",
    "    text = _CLEAN_ZW.sub('', text)          # remove zero-widths\n",
    "    text = _CLEAN_PUA.sub('', text)         # remove private-use (incl. \\uf0b7)\n",
    "    text = _CLEAN_CTRL.sub(' ', text)       # drop stray controls\n",
    "    text = _BULLETS.sub(' • ', text)        # normalize bullets if you want to keep them\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3948c7c-3dd9-4210-bbda-eb81362294b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentence_split(text: str) -> List[Dict]:\n",
    "    text = clean_text(text)\n",
    "    _SPLIT = re.compile(r'(?<=[.!?])\\s+(?=[A-Z0-9\"\\'(])')\n",
    "    _ABBR_END = re.compile(r'\\b(e\\.g|i\\.e|Mr|Ms|Dr)\\.$')\n",
    "    norm = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if not norm:\n",
    "        return []\n",
    "    parts, start = [], 0\n",
    "    for m in _SPLIT.finditer(norm):\n",
    "        parts.append((start, m.start()))\n",
    "        start = m.end()\n",
    "    parts.append((start, len(norm)))\n",
    "\n",
    "    joined: List[Dict] = []\n",
    "    for s, e in parts:\n",
    "        seg = norm[s:e]\n",
    "        if not seg:\n",
    "            continue\n",
    "        if joined and _ABBR_END.search(joined[-1][\"text\"]):\n",
    "            joined[-1][\"text\"] = norm[joined[-1][\"start\"]:e]\n",
    "            joined[-1][\"end\"] = e\n",
    "        else:\n",
    "            joined.append({\"text\": seg, \"start\": s, \"end\": e})\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072efe56-c469-443a-8bf1-dccd7f93b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_overlong_unit(u: Dict, tok, max_tokens: int) -> List[Dict]:\n",
    "    enc = tok(u[\"text\"], add_special_tokens=False, return_offsets_mapping=True)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    offs = enc[\"offset_mapping\"]  # [(start,end) in u[\"text\"]]\n",
    "    out: List[Dict] = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        j = min(i + max_tokens, len(ids))\n",
    "        sub_rel_start = offs[i][0]\n",
    "        sub_rel_end   = offs[j-1][1]\n",
    "        sub_text = u[\"text\"][sub_rel_start:sub_rel_end]\n",
    "        out.append({\n",
    "            \"text\": sub_text,\n",
    "            \"start\": u[\"start\"] + sub_rel_start,\n",
    "            \"end\":   u[\"start\"] + sub_rel_end,\n",
    "            **{k: v for k, v in u.items() if k not in (\"text\",\"start\",\"end\")}\n",
    "        })\n",
    "        i = j\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9815f37e-e878-4afa-b09f-facdbbdc9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_chunks(\n",
    "    units: Sequence[Dict],\n",
    "    tokenizer_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    max_tokens: int = 240,\n",
    "    overlap_tokens: int = 48,\n",
    "    corpus: Optional[str] = None,\n",
    "    carry_keys: Sequence[str] = (\"page\",)\n",
    "    ) -> List[Dict]:\n",
    "    \n",
    "    tok = _get_tokenizer(tokenizer_name)\n",
    "\n",
    "    # 1) Expand any overlong sentences\n",
    "    expanded: List[Dict] = []\n",
    "    src_idx: List[int] = []\n",
    "    for i, u in enumerate(units):\n",
    "        if _tok_len(tok, u[\"text\"]) > max_tokens:\n",
    "            parts = _split_overlong_unit(u, tok, max_tokens)\n",
    "            expanded.extend(parts)\n",
    "            src_idx.extend([i] * len(parts))\n",
    "        else:\n",
    "            expanded.append(u.copy())\n",
    "            src_idx.append(i)\n",
    "\n",
    "    # 2) Precompute token lengths\n",
    "    for u in expanded:\n",
    "        u[\"_tok\"] = _tok_len(tok, u[\"text\"])\n",
    "\n",
    "    # 3) Greedy pack with overlap\n",
    "    chunks: List[Dict] = []\n",
    "    cur: List[Dict] = []\n",
    "    cur_src: List[int] = []\n",
    "    cur_tok = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur, cur_src, cur_tok\n",
    "        if not cur:\n",
    "            return\n",
    "        s0, eN = cur[0][\"start\"], cur[-1][\"end\"]\n",
    "        text = corpus[s0:eN] if corpus is not None else \" \".join(u[\"text\"] for u in cur).strip()\n",
    "\n",
    "        meta = {}\n",
    "        for k in carry_keys:\n",
    "            vals = [u.get(k) for u in cur if u.get(k) is not None]\n",
    "            if vals:\n",
    "                meta[k + \"s\"] = sorted(set(vals))  # e.g., 'pages'\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": len(chunks),\n",
    "            \"text\": text,\n",
    "            \"start\": s0,\n",
    "            \"end\": eN,\n",
    "            \"n_tokens\": cur_tok,\n",
    "            \"unit_indices\": sorted(set(cur_src)),\n",
    "            **meta\n",
    "        })\n",
    "\n",
    "        # build overlap tail\n",
    "        if overlap_tokens > 0:\n",
    "            t = 0\n",
    "            tail, tail_src = [], []\n",
    "            for u, si in zip(reversed(cur), reversed(cur_src)):\n",
    "                tail.insert(0, u); tail_src.insert(0, si)\n",
    "                t += u[\"_tok\"]\n",
    "                if t >= overlap_tokens:\n",
    "                    break\n",
    "            cur, cur_src = tail, tail_src\n",
    "            cur_tok = sum(u[\"_tok\"] for u in cur)\n",
    "        else:\n",
    "            cur, cur_src, cur_tok = [], [], 0\n",
    "\n",
    "    for u, si in zip(expanded, src_idx):\n",
    "        if cur and cur_tok + u[\"_tok\"] > max_tokens:\n",
    "            flush()\n",
    "        cur.append(u); cur_src.append(si); cur_tok += u[\"_tok\"]\n",
    "    flush()\n",
    "\n",
    "    # optional: drop temp fields\n",
    "    for u in expanded:\n",
    "        u.pop(\"_tok\", None)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea11cf3-1c92-4413-891d-c04e2b9bf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk_to_parquet(chunk_uid, text, filename=\"library.parquet\"):\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"chunk_uid\" : chunk_uid,\n",
    "        \"text\" : text\n",
    "    }])\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        new_row.to_parquet(filename, engine=\"fastparquet\", index=False)\n",
    "\n",
    "    else:\n",
    "        df = pd.read_parquet(filename, engine=\"fastparquet\")\n",
    "\n",
    "        if chunk_uid not in df[\"chunk_uid\"].values:\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            df.to_parquet(filename, engine=\"fastparquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d786e87f-a14c-4389-878a-796a5429a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vector(chunk_uid, vector, filename=\"library.pkl\"):\n",
    "    new_row = {\n",
    "        \"chunk_uid\" : chunk_uid,\n",
    "        \"vector_embedding\" : vector\n",
    "    }\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump([new_row], f)\n",
    "        return\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        library = pickle.load(f)\n",
    "\n",
    "    for row in library:\n",
    "        if row[\"chunk_uid\"] == chunk_uid:\n",
    "            if np.allclose(row[\"vector_embedding\"], new_row[\"vector_embedding\"]):\n",
    "                return\n",
    "            else:\n",
    "                row[\"vector_embedding\"] = new_row[\"vector_embedding\"]\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    pickle.dump(library, f)\n",
    "                return\n",
    "\n",
    "    library.append(new_row)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(library, f)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aed1c95-601b-45f1-b558-d866ebbad4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docx(file_path):\n",
    "    return docx2txt.process(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66cad126-cf86-4faf-971d-6d02f6a0039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    \n",
    "    page_texts = []\n",
    "\n",
    "    for p in range(len(doc)):\n",
    "        raw = doc[p].get_text(\"text\")\n",
    "        page_texts.append(clean_text(raw))\n",
    "\n",
    "        SEP = \"\\n\\n\"\n",
    "        corpus = SEP.join(page_texts)\n",
    "\n",
    "        units = []\n",
    "        offset = 0\n",
    "\n",
    "        for page_idx, page_clean in enumerate(page_texts, start=1):\n",
    "            sentences = simple_sentence_split(page_clean)\n",
    "            for s in sentences:\n",
    "                units.append({\n",
    "                    \"text\" : s[\"text\"],\n",
    "                    \"start\" : offset + s[\"start\"],\n",
    "                    \"end\" : offset + s[\"end\"],\n",
    "                    \"page\" : page_idx\n",
    "                })\n",
    "                \n",
    "    return corpus, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32eec8be-b117-4496-b7a7-d4ed8a29a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docx(file_path):\n",
    "    text = parse_docx(file_path) or \"\"\n",
    "    sentences = simple_sentence_split(text)\n",
    "\n",
    "    chunks = pack_chunks(\n",
    "        sentences,\n",
    "        tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_tokens=240,\n",
    "        overlap_tokens=48,\n",
    "        corpus=None,\n",
    "        carry_keys=(\"page\",)\n",
    "    )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c9491ab-d4d8-4566-ab92-c9a068b4ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path):\n",
    "    corpus, units = parse_pdf(file_path)\n",
    "    chunks = pack_chunks(units, corpus)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "62821ec5-3871-4c86-9e61-ffac51ff4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html(file_path_or_url, **kwargs):\n",
    "    if str(file_path_or_url).startswith((\"http://\", \"https://\")):\n",
    "        resp = requests.get(file_path_or_url)\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "    else:\n",
    "        with open(file_path_or_url, \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    sentences = simple_sentence_split(text)\n",
    "    chunks = pack_chunks(sentences)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c00ac4c-1a13-4ed4-81f4-0362b0222902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(ext):\n",
    "    document_pipeline = {\n",
    "        \".pdf\": process_pdf,\n",
    "        \".docx\": process_docx,\n",
    "        \".txt\": None, \n",
    "        \".md\": None, \n",
    "        \".csv\": None, \n",
    "        \".xlsx\": None, \n",
    "        \".pptx\": None,\n",
    "        \".rtf\": None, \n",
    "        \".epub\": None, \n",
    "        \".odt\": None, \n",
    "        \".ods\": None, \n",
    "        \".odp\": None,\n",
    "        \".html\": process_html, \n",
    "        \".json\": None, \n",
    "        \".yml\": None, \n",
    "        \".eml\": None,\n",
    "    }\n",
    "    return document_pipeline.get(ext.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1240b311-0dbc-4595-8a6d-70abac5306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_embedding(text, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(embedding_model,\n",
    "                               cache_folder=\"./models_cache\"\n",
    "                               )\n",
    "    \n",
    "    vector = model.encode(text, convert_to_numpy=True, batch_size=32, show_progress_bar=False)\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6934ec18-6dbe-4466-90da-88d1ffd371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, **kwargs):\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    library_csv = \"library.csv\"\n",
    "    vector_pkl = \"library_vectors.pkl\"\n",
    "    is_url = str(file_path).startswith((\"http://\", \"https://\"))\n",
    "    ext = \".html\" if is_url else Path(file_path).suffix.lower()\n",
    "    \n",
    "    pipeline = get_pipeline(ext)\n",
    "\n",
    "    model = SentenceTransformer(embedding_model,\n",
    "                                cache_folder=\"./models_cache\"\n",
    "                               )\n",
    "    \n",
    "    if pipeline is None:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "        \n",
    "    chunks = pipeline(file_path, **kwargs)\n",
    "\n",
    "    if not os.path.isfile(library_csv):\n",
    "        with open(library_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                \"chunk_uid\",\n",
    "                \"document\",\n",
    "                \"document_type\",\n",
    "                \"n_tokens\",\n",
    "                \"embedding_model\",\n",
    "                \"character_start\",\n",
    "                \"character_end\",\n",
    "                \"timestamp_iso\",\n",
    "                \"timestamp_ms\"\n",
    "            ])\n",
    "\n",
    "    c = 0\n",
    "    for chunk in chunks:\n",
    "        chunk_uid = get_uid(chunk['text'])\n",
    "        \n",
    "        #check if row already exists\n",
    "        df = pd.read_csv(library_csv)\n",
    "        if not df.loc[df['chunk_uid'] == chunk_uid].empty:\n",
    "            continue\n",
    "\n",
    "        vector = model.encode(chunk['text'], convert_to_numpy=True, batch_size=32, show_progress_bar=False)\n",
    "        \n",
    "        timestamp_iso, timestamp_ms = get_iso_time()\n",
    "        \n",
    "        with open(library_csv, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                chunk_uid,\n",
    "                Path(file_path).stem,\n",
    "                ext,\n",
    "                chunk['n_tokens'],\n",
    "                MODEL_NAME,\n",
    "                chunk['start'],\n",
    "                chunk['end'],\n",
    "                timestamp_iso,\n",
    "                timestamp_ms\n",
    "            ])\n",
    "\n",
    "        #save the raw text and the uid into a parquet file\n",
    "        save_chunk_to_parquet(chunk_uid, chunk['text'])\n",
    "        save_vector(chunk_uid, vector)\n",
    "        \n",
    "        c += 1\n",
    "            \n",
    "    print(f\"{c} new chunks added to the library\")\n",
    "                \n",
    "    #return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33d1a113-efc2-48f7-b57a-906d63d6de18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = \"TylerTwohig\"\n",
    "user_input = \"z\"\n",
    "prompt_model = \"DeepSeek-R1:latest\"\n",
    "save_user_input(\"TylerTwohig\", user_input, prompt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7d82ed34-ea1a-4269-8a68-af3ba82c90b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 new chunks added to the library\n"
     ]
    }
   ],
   "source": [
    "process_document(\"https://www.forbes.com/sites/cognitiveworld/2019/03/24/taxonomies-vs-ontologies/?sh=3a6bbac87d53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a75499bb-fff2-4d68-80b1-94e93935a02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new chunks added to the library\n",
      "97 new chunks added to the library\n",
      "289 new chunks added to the library\n",
      "278 new chunks added to the library\n",
      "125 new chunks added to the library\n",
      "128 new chunks added to the library\n",
      "165 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "1 new chunks added to the library\n",
      "61 new chunks added to the library\n",
      "1 new chunks added to the library\n",
      "43 new chunks added to the library\n",
      "61 new chunks added to the library\n",
      "87 new chunks added to the library\n",
      "1 new chunks added to the library\n",
      "59 new chunks added to the library\n",
      "220 new chunks added to the library\n",
      "265 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "40 new chunks added to the library\n",
      "147 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "1 new chunks added to the library\n",
      "115 new chunks added to the library\n",
      "1 new chunks added to the library\n",
      "2653 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "90 new chunks added to the library\n",
      "88 new chunks added to the library\n",
      "0 new chunks added to the library\n",
      "0 new chunks added to the library\n"
     ]
    }
   ],
   "source": [
    "document_directory = \"documents/\"\n",
    "\n",
    "for file in os.listdir(document_directory):\n",
    "    process_document(os.path.join(document_directory, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881985a0-85b2-4ab8-8884-8525a48559c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52406255-5539-45f2-98fb-6678482190ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
